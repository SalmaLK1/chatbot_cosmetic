import logging
import time
import os
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from tiktoken import get_encoding
from .config import INDEX_PATH
from .evaluation import rerank_documents
from .document_processing import chunk_text_semantically, extract_text
from .file_utils import get_title_from_filename, get_file_hash
from .models import db, ChatMessage
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ==============================
# CONFIGURATION DU MOD√àLE MPT
# ==============================

# Mod√®le MPT-7B de MosaicML, version optimis√©e pour le dialogue
# Alternative: "mosaicml/mpt-3b-instruct" pour machines avec RAM limit√©e
MODEL_NAME = "mosaicml/mpt-7b-instruct"

# Initialisation du tokenizer pour le pr√©traitement du texte
# G√®re la tokenization des prompts et des r√©ponses
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Chargement du mod√®le principal avec configuration optimis√©e CPU
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="cpu",            # Sp√©cifique Windows/CPU - pas de support GPU
    torch_dtype=torch.float32,   # Pr√©cision simple pour stabilit√© sur CPU
    low_cpu_mem_usage=True       # R√©duction consommation m√©moire lors du chargement
)

# ==============================
# GESTION DES EMBEDDINGS ET BASE VECTORIELLE FAISS
# ==============================

# Mod√®le d'embedding SentenceTransformers - √©quilibre performance/vitesse
# all-MiniLM-L6-v2: 384 dimensions, rapide et efficace
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Variable globale pour l'index FAISS - initialis√©e √† None au d√©marrage
db_faiss = None

def load_faiss_index():
    """
    Charge l'index FAISS depuis le stockage local.
    Cr√©e un index vide si aucun index existant n'est trouv√©.
    
    Returns:
        None
    """
    global db_faiss
    try:
        # Tentative de chargement de l'index existant
        # allow_dangerous_deserialization=True n√©cessaire pour FAISS mais n√©cessite confiance dans la source
        db_faiss = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
        logging.info("‚úÖ Index FAISS charg√©.")
    except Exception as e:
        # Fallback: cr√©ation d'un nouvel index vide
        logging.warning(f"‚ö†Ô∏è Index non trouv√© ou invalide, cr√©ation d'un index vide : {e}")
        db_faiss = FAISS.from_documents([], embeddings)  # Index vide avec mod√®le d'embedding
        db_faiss.save_local(INDEX_PATH)  # Sauvegarde pour usage futur
        logging.info("‚úÖ Index FAISS vide cr√©√©.")

def reset_faiss_index():
    """
    R√©initialise compl√®tement l'index FAISS.
    Utile pour les tests ou la maintenance.
    
    Returns:
        None
    """
    global db_faiss
    logging.info("‚ö†Ô∏è R√©initialisation de l'index FAISS...")
    # Cr√©ation d'un nouvel index vide
    db_faiss = FAISS.from_documents([], embeddings)
    db_faiss.save_local(INDEX_PATH)  # Persistance imm√©diate
    logging.info("‚úÖ Index FAISS r√©initialis√©.")

def get_existing_document_ids():
    """
    R√©cup√®re tous les IDs de documents pr√©sents dans l'index.
    Utilis√© pour √©viter les doublons lors de l'indexation.
    
    Returns:
        set: Ensemble des IDs de documents existants
    """
    try:
        # Recherche vide pour r√©cup√©rer tous les documents (limit√© √† 1000)
        return set(
            doc.metadata.get("document_id")
            for doc in db_faiss.similarity_search("", k=1000)
            if doc.metadata.get("document_id")  # Filtre les documents sans ID
        )
    except Exception as e:
        logging.warning(f"Erreur r√©cup√©ration des document_id : {e}")
        return set()  # Retourne un set vide en cas d'erreur

def add_document_to_index(text, metadata=None):
    """
    Ajoute un document √† l'index FAISS apr√®s traitement et d√©coupage.
    
    Args:
        text (str): Texte du document √† indexer
        metadata (dict, optional): M√©tadonn√©es associ√©es au document
    
    Returns:
        bool: True si l'ajout r√©ussi, False sinon
    """
    global db_faiss
    try:
        # Validation du texte d'entr√©e
        if not text.strip():
            logging.warning("Texte vide, rien √† indexer.")
            return False

        # Extraction et v√©rification de l'ID du document
        document_id = metadata.get("document_id") if metadata else None
        existing_ids = get_existing_document_ids()
        
        # V√©rification de doublon
        if document_id and document_id in existing_ids:
            logging.info(f"üìõ Document d√©j√† index√© : {document_id}")
            return False

        # D√©coupage s√©mantique du texte en chunks
        # max_tokens=500: taille optimale pour la recherche
        # overlap_tokens=100: pr√©servation du contexte entre chunks
        chunks = chunk_text_semantically(text, max_tokens=500, overlap_tokens=100)
        docs = []
        
        # Cr√©ation des objets Document avec m√©tadonn√©es enrichies
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                "chunk_index": i,  # Position du chunk dans le document
                "chunk_length": len(chunk),  # Longueur en caract√®res
                "title": metadata.get("title", "Sans titre") if metadata else "Sans titre"
            })
            docs.append(Document(page_content=chunk, metadata=chunk_metadata))

        # Ajout √† l'index et sauvegarde
        db_faiss.add_documents(docs)
        db_faiss.save_local(INDEX_PATH)
        logging.info(f"‚úÖ {len(docs)} chunks ajout√©s avec m√©tadonn√©es enrichies.")
        return True
        
    except Exception as e:
        logging.error(f"Erreur ajout document √† l'index : {e}")
        return False

# ==============================
# GESTION DE L'HISTORIQUE DEPUIS LA BASE DE DONN√âES
# ==============================

def get_chat_history_from_db(user_id, session_id, thread_id, limit=10):
    """
    R√©cup√®re l'historique des conversations depuis la base de donn√©es.
    Reconstruit les paires question/r√©ponse pour le contexte.
    
    Args:
        user_id: Identifiant de l'utilisateur
        session_id: Identifiant de session
        thread_id: Identifiant du thread de conversation
        limit (int): Nombre de paires de messages √† r√©cup√©rer
    
    Returns:
        list: Liste de dictionnaires contenant les paires user/assistant
    """
    try:
        # R√©cup√©ration des messages depuis la BDD
        messages = (
            ChatMessage.query
            .filter_by(user_id=user_id, session_id=session_id, thread_id=thread_id)
            .order_by(ChatMessage.created_at.desc())  # Plus r√©cents en premier
            .limit(limit * 2)  # √ó2 car on veut des paires
            .all()
        )
        messages.reverse()  # Remise dans l'ordre chronologique

        # Reconstruction des paires question/r√©ponse
        history_pairs = []
        current_pair = {}
        
        for msg in messages:
            if msg.role == "user":
                current_pair["user"] = msg.message
            elif msg.role == "assistant":
                current_pair["assistant"] = msg.message

            # Paire compl√®te d√©tect√©e
            if "user" in current_pair and "assistant" in current_pair:
                history_pairs.append(current_pair)
                current_pair = {}  # R√©initialisation pour la paire suivante

        return history_pairs
        
    except Exception as e:
        logging.error(f"Erreur r√©cup√©ration historique : {e}")
        return []  # Retourne liste vide en cas d'erreur

# ==============================
# APPEL DU MOD√àLE MPT POUR LA G√âN√âRATION
# ==============================

def call_mpt(prompt, max_tokens=512):
    """
    Ex√©cute le mod√®le MPT avec le prompt fourni.
    
    Args:
        prompt (str): Texte d'entr√©e pour le mod√®le
        max_tokens (int): Nombre maximum de tokens √† g√©n√©rer
    
    Returns:
        str: R√©ponse g√©n√©r√©e par le mod√®le
    """
    try:
        # Tokenization du prompt
        inputs = tokenizer(prompt, return_tensors="pt")  # Tenseurs PyTorch
        
        # G√©n√©ration de la r√©ponse
        outputs = model.generate(**inputs, max_new_tokens=max_tokens)
        
        # D√©codage et nettoyage de la r√©ponse
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
        
    except Exception as e:
        logging.error(f"Erreur MPT : {e}")
        return " R√©ponse impossible."  # Message d'erreur g√©n√©rique

# ==============================
# SYST√àME RAG FUSION AVANC√â
# ==============================

def rag_fusion_multi_docs(query, chat_history=None, k=6, nb_messages=10, use_reranking=True):
    """
    Impl√©mente le syst√®me RAG Fusion avec recherche multi-documents.
    Combine recherche vectorielle, re-ranking et contexte historique.
    
    Args:
        query (str): Question de l'utilisateur
        chat_history (list): Historique des conversations
        k (int): Nombre de documents √† r√©cup√©rer
        nb_messages (int): Nombre de messages d'historique √† inclure
        use_reranking (bool): Active/d√©sactive le re-ranking
    
    Returns:
        tuple: (r√©ponse, documents de contexte)
    """
    # V√©rification de l'initialisation de FAISS
    if db_faiss is None:
        logging.error("Index FAISS non charg√©")
        return " Index non charg√©.", []

    # Initialisation par d√©faut de l'historique
    if chat_history is None:
        chat_history = []

    try:
        # √âtape 1: Recherche vectorielle initiale
        retrieved_docs = db_faiss.similarity_search(query, k=k)
        
        # √âtape 2: Re-ranking des r√©sultats (optionnel)
        docs = rerank_documents(query, retrieved_docs, top_k=k, use_reranking=use_reranking)
        
    except Exception as e:
        logging.error(f"Erreur recherche documentaire : {e}")
        return f"Erreur recherche documentaire : {e}", []

    # Initialisation du tokenizer pour comptage (non utilis√© actuellement)
    tokenizer_gpt = get_encoding("gpt2")
    
    # Construction du contexte documentaire
    context_text = ""
    context_docs = []

    for doc in docs:
        # Formatage des sources avec m√©tadonn√©es
        context_text += f"[Source: {doc.metadata.get('title', 'Document inconnu')}]\n{doc.page_content}\n\n"
        context_docs.append(doc)

    # Construction de l'historique r√©sum√©
    summarized_history = ""
    for turn in chat_history[-nb_messages:]:  # Derniers N messages seulement
        summarized_history += f"Utilisateur : {turn['user']}\nAssistant : {turn['assistant']}\n"

    # Construction du prompt final structur√©
    prompt = f"""
Tu es un assistant IA expert.
Question :
{query}

Contexte documentaire :
{context_text}

Historique r√©sum√© :
{summarized_history}

R√©ponse :
""".strip()

    # Appel au mod√®le pour g√©n√©ration
    full_answer = call_mpt(prompt)
    return full_answer, context_docs

# ==============================
# MODE PROMPT DIRECT (SANS RAG)
# ==============================

def rag_direct_prompt(query, chat_history=None, nb_messages=10):
    """
    Mode sans RAG - utilise seulement l'historique et la question.
    Utile pour les questions g√©n√©rales ne n√©cessitant pas de documentation.
    
    Args:
        query (str): Question de l'utilisateur
        chat_history (list): Historique des conversations
        nb_messages (int): Nombre de messages d'historique √† inclure
    
    Returns:
        str: R√©ponse g√©n√©r√©e par le mod√®le
    """
    # Initialisation par d√©faut
    if chat_history is None:
        chat_history = []

    # Construction de l'historique r√©sum√©
    summarized_history = ""
    for turn in chat_history[-nb_messages:]:
        summarized_history += f"Utilisateur : {turn['user']}\nAssistant : {turn['assistant']}\n"

    # Prompt simplifi√© sans contexte documentaire
    prompt = f"""
Question :
{query}

Historique r√©sum√© :
{summarized_history}

R√©ponse :
""".strip()

    return call_mpt(prompt)

# ==============================
# TRAITEMENT PRINCIPAL DES QUESTIONS
# ==============================

def process_question(user_id, session_id, thread_id, question, use_rag=True, nb_messages=10, use_reranking=True):
    """
    Point d'entr√©e principal pour le traitement des questions.
    G√®re le mode RAG/direct et la persistance en base.
    
    Args:
        user_id: Identifiant de l'utilisateur
        session_id: Identifiant de session
        thread_id: Identifiant du thread
        question (str): Question √† traiter
        use_rag (bool): Active/d√©sactive le mode RAG
        nb_messages (int): Nombre de messages d'historique
        use_reranking (bool): Active/d√©sactive le re-ranking
    
    Returns:
        str: R√©ponse g√©n√©r√©e
    """
    # R√©cup√©ration de l'historique depuis la BDD
    chat_history = get_chat_history_from_db(user_id, session_id, thread_id, limit=nb_messages)

    # S√©lection du mode de traitement
    if use_rag:
        answer, _ = rag_fusion_multi_docs(question, chat_history, nb_messages=nb_messages, use_reranking=use_reranking)
    else:
        answer = rag_direct_prompt(question, chat_history, nb_messages=nb_messages)

    # Persistance de l'√©change en base de donn√©es
    try:
        # Enregistrement de la question utilisateur
        db.session.add(ChatMessage(
            user_id=user_id, 
            session_id=session_id, 
            thread_id=thread_id, 
            role="user", 
            message=question
        ))
        
        # Enregistrement de la r√©ponse de l'assistant
        db.session.add(ChatMessage(
            user_id=user_id, 
            session_id=session_id, 
            thread_id=thread_id, 
            role="assistant", 
            message=answer
        ))
        
        db.session.commit()  # Validation de la transaction
        
    except Exception as e:
        db.session.rollback()  # Annulation en cas d'erreur
        logging.error(f"Erreur enregistrement historique : {e}")

    return answer

# ==============================
# GESTION DES FICHIERS UPLOAD√âS
# ==============================

def handle_uploaded_file(file, user_id, session_id, thread_id, question=None, use_rag=True, nb_messages=10, use_reranking=True):
    """
    Traite un fichier upload√©: extraction, indexation et r√©ponse optionnelle.
    
    Args:
        file: Objet fichier upload√©
        user_id: Identifiant de l'utilisateur
        session_id: Identifiant de session
        thread_id: Identifiant du thread
        question (str, optional): Question associ√©e au fichier
        use_rag (bool): Active/d√©sactive le mode RAG
        nb_messages (int): Nombre de messages d'historique
        use_reranking (bool): Active/d√©sactive le re-ranking
    
    Returns:
        str: Message de confirmation ou r√©ponse √† la question
    """
    # Extraction du texte depuis le fichier
    text = extract_text(file)
    
    # V√©rification de la r√©ussite de l'extraction
    if not text or "Erreur" in text:
        return text  # Retourne l'erreur d'extraction

    # Pr√©paration pour le hachage du fichier
    file.seek(0)
    file_bytes = file.read()
    file.seek(0)  # Reset pour usage futur
    
    # G√©n√©ration d'ID unique bas√© sur le contenu
    doc_id = get_file_hash(file_bytes)
    title = get_title_from_filename(file.filename)

    # M√©tadonn√©es pour l'indexation
    metadata = {
        "document_id": doc_id,  # ID unique pour d√©duplication
        "source": file.filename,  # Nom original du fichier
        "title": title  # Titre extrait du nom de fichier
    }

    # Indexation du document
    add_document_to_index(text, metadata=metadata)

    # R√©ponse √† une question si fournie
    if question:
        return process_question(user_id, session_id, thread_id, question, use_rag, nb_messages, use_reranking)
    
    return " Fichier index√© avec succ√®s."  # Message de confirmation simple